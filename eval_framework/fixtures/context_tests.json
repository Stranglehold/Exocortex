{
  "fixtures_version": "1.0.0",
  "module": "context_sensitivity",
  "tests": [
    {
      "test_id": "ctx_001",
      "base_task": "Write a Python function that reverses a string",
      "quality_metric": "code_correctness",
      "expected_output_contains": ["def ", "return", "reverse"],
      "context_layers": [
        {"name": "bst_enrichment", "approx_tokens": 200, "content": "[TASK CONTEXT]\n  domain: codegen\n  confidence: 0.90\n  language: python\n  operation: create\n[INSTRUCTION]\n  Write a clean Python function. Use type hints."},
        {"name": "recalled_memories", "approx_tokens": 500, "content": "[RECALLED MEMORIES]\n  - User prefers descriptive variable names over single letters\n  - User's project uses Python 3.11\n  - Previous code review feedback: always include docstrings\n  - User prefers f-strings over .format()\n  - Team convention: functions under 20 lines"},
        {"name": "graph_node", "approx_tokens": 300, "content": "[WORKFLOW NODE: implement]\n  You are executing step 3 of 5 in the Implementation Workflow.\n  Current task: Implement the string utility function.\n  After completion, tests will be run automatically.\n  Focus on correctness over performance."},
        {"name": "role_profile", "approx_tokens": 400, "content": "[ROLE: Code Generation Specialist]\n  Your primary function is generating clean, tested code.\n  Quality standards: PEP 8, type hints, docstrings.\n  Error handling: raise descriptive exceptions.\n  Testing: suggest test cases after implementation.\n  Review: self-review before presenting code."},
        {"name": "personality", "approx_tokens": 600, "content": "[PERSONALITY: Major Zero]\n  Communication style: Systematic and methodical.\n  Present solutions step-by-step. Begin with analysis of the requirement.\n  Follow with implementation. End with verification.\n  Maintain professional tone. Use precise technical language.\n  When uncertain, state assumptions explicitly.\n  Prefer completeness over brevity."},
        {"name": "padding_noise", "approx_tokens": 2000, "content": "[SYSTEM CONTEXT]\n  Organization: Development Team Alpha\n  Active projects: Project Atlas (web platform), Project Beacon (mobile app), Project Cascade (data pipeline)\n  Team members: Alice (frontend), Bob (backend), Carol (DevOps), Dave (QA)\n  Sprint: Sprint 23 — Focus on performance optimization\n  Recent decisions: Migrated from MySQL to PostgreSQL, adopted Redis for caching, switched CI from Jenkins to GitHub Actions\n  Infrastructure: AWS us-east-1, 3 EC2 instances, RDS PostgreSQL, ElastiCache Redis, S3 for static assets\n  Deployment schedule: Staging deploys daily at 2pm, production deploys Tuesdays and Thursdays\n  Coding standards document: Updated Q4 2025, includes sections on error handling, logging, testing requirements\n  API versioning: v2 is current, v1 deprecated but still serving 15% of traffic\n  Monitoring: Datadog for APM, PagerDuty for alerts, Slack #alerts channel\n  Recent incidents: Feb 10 — Redis cache stampede during peak hours, resolved by adding jitter to TTL\n  Technical debt: Legacy authentication module needs refactoring, estimated 2 sprints\n  Dependencies: FastAPI 0.109, SQLAlchemy 2.0, Pydantic v2, pytest 8.0\n  Security: Last pentest Q3 2025, all critical findings resolved, 2 medium findings in backlog\n  Performance targets: p99 latency under 500ms, error rate under 0.1%, availability 99.9%\n  Documentation: ADRs in /docs/decisions/, API docs auto-generated via FastAPI\n  This context block contains additional organizational information that may or may not be relevant to the current task. It serves as background context for the agent's operation within the team structure."}
      ]
    },
    {
      "test_id": "ctx_002",
      "base_task": "Write a bash script that backs up a PostgreSQL database",
      "quality_metric": "code_correctness",
      "expected_output_contains": ["pg_dump", "backup", "#!/bin/bash"],
      "context_layers": [
        {"name": "bst_enrichment", "approx_tokens": 200, "content": "[TASK CONTEXT]\n  domain: devops\n  confidence: 0.88\n  operation: create\n  target: backup_script\n[INSTRUCTION]\n  Create a reliable backup script.\n  Include timestamp in filename and error checking."},
        {"name": "recalled_memories", "approx_tokens": 500, "content": "[RECALLED MEMORIES]\n  - Database host: db.internal.corp:5432\n  - Database name: production_app\n  - Backup directory: /backups/postgres/\n  - Previous backup script had no error handling — caused silent failures\n  - Retention policy: keep 30 days of daily backups\n  - User requested compression (gzip) last time"},
        {"name": "graph_node", "approx_tokens": 300, "content": "[WORKFLOW NODE: implement]\n  Step 2 of 4 in Backup Automation Workflow.\n  Current: Create the backup script.\n  Next: Set up cron job (do NOT do this now).\n  Scope: Script only, no scheduling."},
        {"name": "role_profile", "approx_tokens": 400, "content": "[ROLE: Infrastructure Specialist]\n  Focus on reliability and observability.\n  All scripts must: log to stdout/stderr, return proper exit codes,\n  handle signals (SIGTERM, SIGINT), validate inputs.\n  Prefer: set -euo pipefail, shellcheck-clean code.\n  Security: never hardcode credentials, use env vars or secrets manager."},
        {"name": "personality", "approx_tokens": 600, "content": "[PERSONALITY: Major Zero]\n  Systematic approach. Begin with requirements analysis.\n  Implement with defensive coding. End with usage documentation.\n  Professional and thorough. Cover edge cases proactively."},
        {"name": "padding_noise", "approx_tokens": 2000, "content": "[SYSTEM CONTEXT]\n  Organization: Operations Team Beta\n  Server inventory: 12 EC2 instances, 3 RDS clusters, 2 ElastiCache nodes\n  Backup history: Last 90 days all successful, before that 3 failures in November due to disk space\n  Related scripts: /opt/scripts/backup_redis.sh, /opt/scripts/rotate_logs.sh\n  Monitoring: Nagios checks backup age, alerts if > 26 hours old\n  Compliance: SOC2 requires encrypted backups, HIPAA requires 7-year retention for PHI databases\n  Disk space: /backups volume is 500GB, currently 60% full, projected full in 45 days\n  Network: backup traffic should use the dedicated backup VLAN (10.0.5.0/24)\n  Team on-call rotation: Week 1: Carol, Week 2: Eve, Week 3: Frank, Week 4: Grace\n  Change management: All scripts require PR review by 2 team members, tested in staging first\n  Previous incidents: Jan 5 — backup corruption due to concurrent vacuum, resolved by adding exclusive lock\n  Related Jira tickets: OPS-1234 (backup encryption), OPS-1235 (cross-region replication), OPS-1236 (backup monitoring dashboard)\n  Additional infrastructure details that provide organizational context but are not directly relevant to writing a single backup script. This padding tests whether the model can focus on the essential task despite surrounding noise."}
      ]
    },
    {
      "test_id": "ctx_003",
      "base_task": "Write a React component that displays a sortable data table",
      "quality_metric": "code_correctness",
      "expected_output_contains": ["function", "return", "table", "sort"],
      "context_layers": [
        {"name": "bst_enrichment", "approx_tokens": 200, "content": "[TASK CONTEXT]\n  domain: codegen\n  confidence: 0.85\n  language: javascript\n  operation: create\n  target: react_component\n[INSTRUCTION]\n  Create a reusable table component with sort functionality."},
        {"name": "recalled_memories", "approx_tokens": 500, "content": "[RECALLED MEMORIES]\n  - Project uses React 18 with TypeScript\n  - Team uses functional components, no class components\n  - State management: React Query for server state, useState/useReducer for local\n  - Styling: Tailwind CSS, no CSS modules\n  - Previous table component was too tightly coupled — make this one generic"},
        {"name": "graph_node", "approx_tokens": 300, "content": "[WORKFLOW NODE: implement]\n  Step 2 of 4: Implement table component.\n  Props: data (array of objects), columns (column definitions).\n  Do NOT add pagination yet (that's step 3)."},
        {"name": "role_profile", "approx_tokens": 400, "content": "[ROLE: Frontend Specialist]\n  React best practices: memo for expensive renders, keys for lists.\n  Accessibility: ARIA labels, keyboard navigation, screen reader support.\n  Performance: avoid re-renders, use useCallback for event handlers.\n  Testing: suggest component test cases."},
        {"name": "personality", "approx_tokens": 600, "content": "[PERSONALITY: Methodical]\n  Step 1: Define component interface (props, types).\n  Step 2: Implement core rendering.\n  Step 3: Add interactivity (sorting).\n  Step 4: Style with Tailwind.\n  Present clean, production-ready code."},
        {"name": "padding_noise", "approx_tokens": 2000, "content": "[SYSTEM CONTEXT]\n  Project structure: /src/components/, /src/hooks/, /src/utils/, /src/types/\n  Design system: custom tokens in tailwind.config.js, spacing scale: 4px base\n  Browser support: Chrome 90+, Firefox 88+, Safari 14+, Edge 90+\n  Build: Vite 5.x, TypeScript 5.3, React 18.2\n  Related components: Button.tsx, Modal.tsx, Pagination.tsx, SearchBar.tsx\n  API contract: GET /api/data returns {items: Array<{id, name, email, role, created_at}>}\n  Performance budget: First Contentful Paint < 1.5s, Time to Interactive < 3s\n  Accessibility audit: Last run showed 3 issues — missing alt text, low contrast, no skip link\n  Team members: Alice (lead), Bob (junior), Carol (design), Dave (backend)\n  Sprint board: 5 tasks in progress, 3 in review, 12 in backlog\n  Recent PRs: #234 (search feature), #235 (auth flow), #236 (dashboard layout)\n  Code review guidelines: max 400 lines per PR, tests required, no console.log in production\n  Deployment: Vercel, preview deploys on PR, production on main merge\n  Analytics: Mixpanel for user events, Sentry for error tracking\n  This organizational context provides background but should not significantly affect the component implementation."}
      ]
    },
    {
      "test_id": "ctx_004",
      "base_task": "Explain why this Python code has a memory leak and suggest a fix:\n\ndef process_data(items):\n    results = []\n    for item in items:\n        obj = HeavyObject(item)\n        results.append(obj)\n        obj.callback = lambda: results.append(obj)\n    return results",
      "quality_metric": "explanation_quality",
      "expected_output_contains": ["closure", "reference", "circular", "callback"],
      "context_layers": [
        {"name": "bst_enrichment", "approx_tokens": 200, "content": "[TASK CONTEXT]\n  domain: bugfix\n  confidence: 0.82\n  operation: diagnose\n  error_type: memory_leak\n[INSTRUCTION]\n  Analyze the memory leak. Identify circular references.\n  Suggest fix with weakref or explicit cleanup."},
        {"name": "recalled_memories", "approx_tokens": 500, "content": "[RECALLED MEMORIES]\n  - User's codebase had a similar closure leak in event_handler.py (fixed Feb 2025)\n  - User is familiar with gc module but prefers structural fixes over gc.collect()\n  - HeavyObject is a large ML model wrapper (~500MB per instance)\n  - Previous advice: use __del__ carefully, prefer context managers"},
        {"name": "graph_node", "approx_tokens": 300, "content": "[WORKFLOW NODE: diagnose]\n  Step 1 of 3 in Bug Fix Workflow.\n  Diagnose the memory leak first. Do NOT fix yet.\n  Explain the root cause clearly."},
        {"name": "role_profile", "approx_tokens": 400, "content": "[ROLE: Backend Specialist]\n  Deep Python expertise. Memory management, profiling, optimization.\n  Explain complex concepts clearly. Use diagrams when helpful.\n  Always consider: GC implications, reference counting, weakref patterns."},
        {"name": "personality", "approx_tokens": 600, "content": "[PERSONALITY: Analytical]\n  Approach: Dissect the problem systematically.\n  1. Identify what's happening at the language level.\n  2. Explain the mechanism (closures, references, GC).\n  3. Show the fix with before/after comparison.\n  Use precise technical language. Reference CPython internals when relevant."},
        {"name": "padding_noise", "approx_tokens": 2000, "content": "[SYSTEM CONTEXT]\n  Application: ML pipeline processing 10K items/hour, each creating HeavyObject instances\n  Infrastructure: 64GB RAM server, currently using 58GB (90%+ utilization)\n  Monitoring: RSS growth of 2GB/hour, OOM kills expected within 3 hours\n  Related files: process_data.py, heavy_object.py, pipeline.py, config.py\n  Dependencies: numpy 1.26, torch 2.1, transformers 4.36, pandas 2.1\n  Previous optimizations: batch processing (Sprint 20), memory-mapped files (Sprint 21)\n  Profiling data: objgraph shows 50K+ HeavyObject instances not collected\n  Stack trace: process_data called from pipeline.run() which runs in asyncio event loop\n  Team discussion: considered switching to generators but callback pattern is required by downstream\n  Architecture decisions: ADR-15 chose callbacks over events for real-time processing\n  Performance targets: process 10K items under 1 hour, max 32GB RSS\n  Deployment: Kubernetes pod with 64GB limit, currently on node pool 'ml-workers'\n  Recent changes: upgraded transformers from 4.33 to 4.36 — leak appeared after upgrade\n  This extended context tests whether analysis quality degrades under information overload."}
      ]
    },
    {
      "test_id": "ctx_005",
      "base_task": "Create a REST API endpoint in FastAPI that accepts a JSON body with 'name' and 'email' fields, validates them, and returns a success response",
      "quality_metric": "code_correctness",
      "expected_output_contains": ["FastAPI", "def ", "name", "email", "return"],
      "context_layers": [
        {"name": "bst_enrichment", "approx_tokens": 200, "content": "[TASK CONTEXT]\n  domain: codegen\n  confidence: 0.92\n  language: python\n  framework: fastapi\n  operation: create\n[INSTRUCTION]\n  Create a FastAPI endpoint with Pydantic validation.\n  Return appropriate HTTP status codes."},
        {"name": "recalled_memories", "approx_tokens": 500, "content": "[RECALLED MEMORIES]\n  - User's API follows REST conventions strictly\n  - Previous endpoints use Pydantic v2 model_validator\n  - Email validation: use EmailStr from pydantic\n  - Response format: {\"status\": \"success\", \"data\": {...}}\n  - Error format: {\"status\": \"error\", \"detail\": \"...\"}"},
        {"name": "graph_node", "approx_tokens": 300, "content": "[WORKFLOW NODE: implement]\n  Step 3 of 6: Create user registration endpoint.\n  Route: POST /api/v2/users\n  After this: write tests (step 4)."},
        {"name": "role_profile", "approx_tokens": 400, "content": "[ROLE: API Specialist]\n  RESTful design. Proper status codes. Input validation.\n  Security: rate limiting headers, input sanitization.\n  Documentation: include OpenAPI description strings.\n  Testing: suggest request examples."},
        {"name": "personality", "approx_tokens": 600, "content": "[PERSONALITY: Precise]\n  Present the endpoint with: model definition, route handler, validation logic, response format.\n  Include edge cases: duplicate email, invalid format, empty fields.\n  Show example request and response."},
        {"name": "padding_noise", "approx_tokens": 2000, "content": "[SYSTEM CONTEXT]\n  API documentation: 47 endpoints across 8 resource groups\n  Authentication: JWT with refresh tokens, 15-minute access token TTL\n  Database: PostgreSQL 15 via SQLAlchemy 2.0 async\n  Middleware stack: CORS, rate limiting (100 req/min), request logging, error handler\n  Related endpoints: GET /api/v2/users, PUT /api/v2/users/{id}, DELETE /api/v2/users/{id}\n  Schema migrations: Alembic, 156 migrations total, latest: add_preferences_column\n  Testing: pytest-asyncio, httpx for async client, factory_boy for fixtures\n  CI/CD: GitHub Actions, runs lint + type check + tests + security scan\n  Performance: p50 = 45ms, p99 = 280ms for user endpoints\n  Dependencies: fastapi 0.109, pydantic 2.5, sqlalchemy 2.0, uvicorn 0.25\n  Rate limiting: 100/min for authenticated, 20/min for unauthenticated\n  CORS: allow origins from *.example.com, methods: GET, POST, PUT, DELETE\n  Logging: structured JSON logs to stdout, shipped to Elasticsearch\n  Monitoring: Prometheus metrics, Grafana dashboards\n  This context provides API infrastructure details that are not directly needed for implementing a single endpoint."}
      ]
    }
  ]
}
